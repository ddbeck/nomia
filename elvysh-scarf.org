# TODO title
* S High level executive summary
* Deep dive into Elvysh
#+BEGIN_QUOTE
There are only two hard things in Computer Science: cache invalidation and naming things.

  ---Phil Karlton
#+END_QUOTE
You're about to start working on a service that you haven't worked on before. You point your editor to the source file you need to work on, and after a slight delay for it to be downloaded for the first time you have it open in front of you. You start modifying the code, and after a minute or two for the library dependencies to be downloaded syntax checking kicks in and points out a typo a few lines above. You save your change and jump over to another file that depends on it (which opens immediately) and start making changes there. At first your editor warns you about undefined references when you mention the new code you added, but after a few seconds for the first module to compile those errors go away. You point your browser to the local service URL and get a page saying that the service is being spun up and showing the progress on that. You notice a DB dump is being loaded and you know that will take a while, so you switch to another project, a compute pipeline. When you enter its environment you notice a huge merge happened overnight, you were up to date with master so you don't have any conflicts but you're not sure if the merge impacted the parts of the codebase you care about. A colleague recommends you try out Meld to review the diff, you run a command to launch it and (after a minute of downloading, since you've never used it before) the window pops open and you confirm the merge was fine. You're testing out an algorithm tweak that should only impact a small portion of the parallel work units of the pipeline, after you finish implementation you kick off a run of the pipeline expected to be the same as that day's daily prod run except with just your changes in place, dry-running first to validate that only a small subset of the job is impacted. While that's running, you switch back to your browser and see the service is loaded, so you check your work and, satisfied, open a PR, which triggers an automention of the QA team with a URL to test out. You notice the pipeline run isn't quite done, so you turn off your computer and head out to lunch. When you get back, you see that the QA team reviewed your fix and it looks good and that the pipeline run is done, so you merge your changes to the first project and open up a comparison of the real prod run's results with your test run. After confirming the new results seem better, you open a PR with your proposed changes and head home.

To some of you, this may sound like a utopian dream. To others, a secret nightmare where all of the magic and implicit assumptions will inevitably cause a catastrophic break or, worse, subtle bugs missed until it's too late. A lucky few might have some subset of this available in some form in their domain. No one has it all... yet.

The vision outlined here is not impossible. It's not inherently unreliable, or brittle, or limited to a few special use cases. With elvysh, we can dramatically reduce manual work, increase efficiency, and ensure correctness in almost any domain where computers are used. Read on to find out how.
** Names and substitution
Naming and substitution are ubiquitous[fn:church] in computation, and many systems end up dealing with them explicitly. Compilers take module names and substitute in appropriate symbol tables. Browsers take URLs and substitute in appropriate web sites. Package managers take package names and substitute in appropriate changes to your environment. In other cases, the system has no explicit handle on names but the user or programmer fills in: We refer to other pieces of code, or techniques, or other computations, or data sources, or a million other things by name, and in implementation fill in special-case substitution of that name that preserves the intended meaning[fn:hope].

These domans are, of course, very different. Browsers don't know what a symbol table is, and installing a package is very different from translating source code to object code. But there are many conceptual commonalities between them, commonalities which in principle could allow for shared implementation and semantics. Unfortunately, most of the time the required functionality is reimplemented from scratch. Like any missed opportunity for reuse, this duplicates work and bugs, leaves many implementations incomplete with respect to functionality or performance, and increases cognitive overhead for users and developers. In this case we also miss opportunities for *cross*-system composition: My package manager may know how to install libpq, and my compiler may know how to resolve libpq-fe.h to a library once it's installed, but there's no general-purpose way to note that the one name links to the other. With the commonalities abstracted into a shared component, system implementers can focus on their domain expertise, and users can benefit from correctness, efficiency, and a coherent, easy to use experience across all of their systems.

Elvysh provides that shared conceptual model and implementation.

[fn:church] If you take the [[https://en.wikipedia.org/wiki/Lambda_calculus][Church]] side of the [[https://en.wikipedia.org/wiki/Church%E2%80%93Turing_thesis][Church-Turing thesis]], name substitution is what computation *is*.
[fn:hope] We hope!

*** TODO Link to model/impl sections?
** The elvysh model [fn:cat]
Elvysh is based off of several core concepts and their interrelations[fn:poly].

[fn:cat] Elvysh's model is based off of structures borrowed from category theory. No category theory is needed to understand this section, but footnotes will be included for those with the background.
[fn:poly] Many of the concepts come together to form a particular kind of lax 2-polycategory.
*** Resources
*Resources* are a very general concept, anything that can be instantiated, used, consumed, etc. A program that prints out "Hello", a website to check the weather, a block of memory containing the result of some computation, a Kafka cluster, and so on can all be considered resources. Resources have *affordances* that are exploited via *capabilities* (or *handles*): reads from memory go via a pointer, the weather page is loaded via a connection, the program is executed via a file descriptor or path within a directory.

Resources have *types*[fn:0-cell]. These might be something general like "an HTTP service" or something more specific like "an immutable regular file". Types determine the affordances of the resource: a regular file can be read from, a program can be executed. Some types are *subtypes* of others, in that affordances of the *supertype* are also exploitable on resources of the subtype. Any (unix) file can have stat(2) called on it, including immutable regular files.

Resource types also determine *equivalence classes*, collections of resources that can be considered "the same" as each other. This is a very domain-specific semantic notion: two equivalent immutable regular files must have the same contents but not necessarily the same inode numbers, and two equivalent web services can vary in number of backing servers or even implementation language so long as they respond to requests in the same way. Note that subtyping does /not/ necessarily preserve equivalence; two "hello" programs that only differ at some padding byte are equivalent /as programs/ but not as regular files. This is in fact one of the use cases for defining a new resource type: if a "package" is a collection of programs, libraries, documentation, etc. that affords installation into an environment, a "cross-agnostic package" might have the same affordances but fail to distinguish packages that are the same except one was compiled natively and the other cross-compiled.

[fn:0-cell] The 0-cells of the polycategory. Note that we do not in general identify a specific resource with some point of the relevant 0-cell, to be explained in the next section.
*** Names
A *name*[fn:1-cell] is process that consumes and produces resources. A name has a sequence of *inputs*, which are resource types, such that an appropriate resource for /every/ input must be provided to run the process. A name also has a sequence of *outputs*, also resource types, such that an appropriate resource for /some/ output will be produced when the process runs[fn:multiple]. For example, the name "cat2" might take two inputs that are readable files and have one output, another readable file, corresponding to the concatenation of the inputs. This can be visualized as:

[FIG cat2]

Names must respect resource equivalence, in the sense that 

Names with no inputs and a single output are also called *named resources*[fn:points], since they correspond directly to the resource produced when the name is run.

+ Composition
+ Inlining
+ Structural
+ Contextual
+ Caching
+ Forwarding (namespaces?)

[fn:1-cell] The 1-cells of the polycategory.
[fn:multiple] Note that this is /not/ the same as a function returning multiple values, or Nix's multiple outputs. Only /one/ resource will be produced, whose type will match /one/ of the outputs; to have a single name refer to multiple resources a collection resource type (e.g. a map from output name to resource) can be used.
[fn:points] /These/ are the points of the relevant 0-cell. Not every resource has a name that fits the requirements of names generally, at least not obviously so, so while every named resource corresponds to some resource the converse isn't true.
              Much of the time, names will only have a single output.
**** TODO Figures
*** Prior work
+ CAS (git, IPFS)
+ Nix
+ Unison
+ Nelson
** L The core theoretical model
+ Polycategories
  + Substructural (e.g. pipes)
  + References as un-cut compositions (cut elim/ref trans)
+ Reductions (laxity)
  + Nix example(project to output â†’ reduce to outpath)
+ Caching
  + Store forwarding
+ Naming
  + Hashing vs authoritative name server, what to hash
+ Indexicality/subindex
** L Core technical components/architecture implementing the model
+ GC
+ Centralize reductions/per user
+ Lazy/incomplete
** M Potential applications (general, not Scarf specific)
+ Cross comp
+ Modules/functions/computation (Unison)
+ Pipeline
+ Packages
+ Services
+ Compliation
+ Memory map
** S Engineering standards/technical philosophy of the implementation
+ Spec
+ Composable (lib/framework)
  + Mechanism vs policy (semantic)
+ O11Y (dynamic adjustment)
+ Verification
+ Caps (resource limits)
+ Poly/mono repo, schemas
* Scarf porcelain
** S Why Elvysh is the right basis for Scarf's tooling
+ Provide a package distribution channel that collected usage statistics for maintainers by default (better-informed maintainers -> better software, enables business decisions around OSS)
+ Give OSS authors enough leverage over their own code to meaningfully charge the companies that rely on it
+ Provide a commercial platform for OSS delivery to commercial users, by offering native payments, paid licensing, premium feature delivery, etc
+ Provide a unified package management experience across different systems
+ Align dev tools around maintainers
+ O11Y â†’ metrics
** S Why scarf is good for elvysh
** M Potential functionality and use cases of frontend(s)
+ Command not found/implicit env (w/locking?)
*** Match domain-specific tooling
** M Expected user knowledge/background for various use cases
(incl setup/config)
** M Expected interface with Elvysh core
** M Accompanying infrastructure
* Project plan
** L Roadmap with technical and functional milestones
Nixpkgs compat:
  Add files
    direct add to store
    Builtin drvs
    recursive vs flat
  References
  Run drvs
    Basic execution
    Funky special features
    Serialize drvs
    Intensional?
    Recursive?
    Remote?
    Substitution?
  GC
  nixexpr interface
    Basic eval
    String context
    path
    derivationStrict
    funky builtins?
    Interface to other stores?
  nixenv/profile interface
    GC connected to profile dirs
Haskell
  Individual module
  Whole package
  Deps?
  nix bidi interaction
Interface
  C
  Rust
  Haskell
Documentation
  Reference/protocols
  Tutorials
  Cookbook/how-to
Formal modelling
Portability?
** L Detailed review of each phase
** L Timelines
** S Opportunities for parallelism/team work
** M Proposal for messaging/marketing to existing Nix and developer tool communities
** M Expected limitations of each milestone and the completed initial product
** S Future opportunities
* S Proposed terms of employment
** Governance
Owner's interest, maintainers decisision

7S, 8M, 5L
