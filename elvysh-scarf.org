#+TITLE: Elvysh and Scarf
#+OPTIONS: H:5
#+OPTIONS: toc:nil
Scarf's strategy centers around a unified package distribution platform and capturing analytics for maintainers. This document introduces elvysh, a technical system that can provide the underpinnings of those capabilities, and how we can use it to ensure a Scarf program on every machine.
* Deep dive into Elvysh
#+BEGIN_QUOTE
There are only two hard things in Computer Science: cache invalidation and naming things.

  ---Phil Karlton
#+END_QUOTE
Naming and substitution are ubiquitous[fn:church] in computation, and many systems end up dealing with them explicitly. Compilers take module names and substitute in appropriate symbol tables. Browsers take URLs and substitute in appropriate web sites. Package managers take package names and substitute in appropriate changes to your environment. In other cases, the system has no explicit handle on names but the user or programmer fills in: We refer to other pieces of code, or techniques, or other computations, or data sources, or a million other things by name, and in implementation fill in special-case substitution of that name that preserves the intended meaning[fn:hope].

These domans are, of course, very different. Browsers don't know what a symbol table is, and installing a package is distinct from translating source code to object code. But there are many conceptual commonalities between them, commonalities which in principle could allow for shared implementation and semantics. Unfortunately, most of the time the required functionality is reimplemented from scratch. Like any missed opportunity for reuse, this duplicates work and bugs, leaves many implementations incomplete with respect to functionality or performance, and increases cognitive overhead for users and developers. In this case we also miss opportunities for *cross*-system composition: My package manager may know how to install libpq, and my compiler may know how to resolve libpq-fe.h to a library once it's installed, but there's no general-purpose way to note that the one name links to the other. With the commonalities abstracted into a shared component, system implementers can focus on their domain expertise, and users can benefit from correctness, efficiency, and a coherent, easy to use experience across all of their systems.

You're about to start working on a service that you haven't worked on before. You point your editor to the source file you need to work on, and after a slight delay for it to be downloaded for the first time you have it open in front of you. You start modifying the code, and after a minute or two for the library dependencies to be downloaded syntax checking kicks in and points out a typo a few lines above. You save your change and jump over to another file that depends on it (which opens immediately) and start making changes there. At first your editor warns you it's not up-to-date but there's an undefined reference when you mentionend the new code you added, but after a few seconds for the first module to compile those errors go away. You finish the work and point your browser to the local service URL and get a page saying that the service is being spun up and showing the progress on that. You notice a DB dump is being loaded and you know that will take a while, so you switch to another project, a compute pipeline. When you enter its environment you notice a huge merge happened overnight, you were up to date with master so you don't have any conflicts but you're not sure if the merge impacted the parts of the codebase you care about. A colleague recommends you try out Meld to review the diff, you run a command to launch it and (after a minute of downloading, since you've never used it before) the window pops open and you confirm the merge was fine. You're testing out an algorithm tweak that should only impact a small portion of the parallel work units of the pipeline, after you finish implementation you kick off a run of the pipeline expected to be the same as that day's daily prod run except with just your changes in place, dry-running first to validate that only a small subset of the job is impacted. While that's running, you switch back to your browser and see the service is loaded, so you check your work and, satisfied, open a PR, which triggers an automention of the QA team with a URL to test out. You notice the pipeline run isn't quite done, so you turn off your computer and head out to lunch. When you get back, you see that the QA team reviewed your fix and it looks good and that the pipeline run is done, so you merge your changes to the first project and open up a comparison of the real prod run's results with your test run. After confirming the new results seem better, you open a PR with your proposed changes and head home.

To some of you, this may sound like a utopian dream. To others, a secret nightmare where all of the magic and implicit assumptions will inevitably cause a catastrophic break or, worse, subtle bugs missed until it's too late. A lucky few might have some subset of this available in some form in their domain. No one has it all... yet.

The vision outlined here is not impossible. It's not inherently unreliable, or brittle, or limited to a few special use cases. With appropriate use of names and a common system for substitution, we can dramatically reduce manual work, increase efficiency, and ensure correctness in almost any domain where computers are used. Elvysh provides the shared conceptual model and common implementation to make it real.

[fn:church] If you take the [[https://en.wikipedia.org/wiki/Lambda_calculus][Church]] side of the [[https://en.wikipedia.org/wiki/Church%E2%80%93Turing_thesis][Church-Turing thesis]], name substitution is what computation *is*.
[fn:hope] We hope!
** The conceptual model
Elvysh models[fn:cat] several core concepts and their interrelations[fn:mon]. In this section, we'll revisit the developer's dream from the introduction and progressively build up the model as a lens to understand one way it might have happened.

[fn:cat] Elvysh's model is based off of structures borrowed from category theory. No category theory is needed to understand this section, but footnotes will be included for those with the background.
[fn:mon] Many of the concepts come together to form a particular kind of monoidal 2-category
*** Resources
In your hypothetical work day, you exploited a number of *resources*: The source files you edited, the modules you imported, the web service you tested. Your usage took advantage of various *affordances* exposed via *capabilities* (or *handles*): Your shell /executed/ meld via its /path in a directory/, the pipeline run comparison tool used /connections to the respective results tables/ to /read/ in the results (or maybe the results directly afford higher level comparison primitives).

The individual resources used can be seen as instances of *resource types*[fn:0-cell]: the piece of the algorithm you modified can be seen as a /pipeline component/, the site the QA team evaluated was a /test web service/. Resource types associate *semantics* to the affordances of their inhabitants, often in terms of abstract *resource state*: an immutable readable file has its /contents/, a (finite) sequence of bytes, and sequential reads of the file return successive portions of those contents. Resource types also induce *equivalence classes* on their resources: Your dry-run of the pipeline confirmed that most of the results of your test run were equivalent (as compute results!) to the corresponding results of the prod run that day. The same resource can be seen as a member of multiple types: Your editor treated the first file you edited as a mutable single-writer regular file, while your compiler may have viewed it as a sequentially readable file. Some types are *subtypes* of others in that a resource of the subtype can also be seen as a resource of the *supertype*, in a way that preserves affordances but not necessarily semantics or equivalence: You can call "stat" on any Unix file and any immutable readable Unix file, but Unix files in general have no notion of contents (think of a socket) and two equivalent immutable readable files may have e.g. different inode numbers and thus be seen as different as Unix files.

Note that resource types and their associated semantics can be very domain-specific. Suppose the compute pipeline is written in C++ and your CI system uses gcc for performance but you prefer clang locally for the better error messages. The object files produced by the two compilers can be quite different, even viewed as object files, and so na√Øvely the object files compiled by CI after the big merge wouldn't be equivalent to the object files you'd compile locally. But viewed as "object files exporting the right symbols following the right platform ABI based on the relevant headers", they can be considered the same.

[fn:0-cell] The (generators of the) 0-cells of the category. Note that we do not in general identify a specific resource with some point of the relevant 0-cell, in part because there is no 1:1 mapping between a resource and its type and in part for reasons detailed in the next section.
*** Names
Each of the resources you utilized were referenced by a *name*[fn:1-cell]: "meld" names a particular program, "the test site for the PR you opened" names a particular web service. More generally, a name can be a procedure relating a finite sequence of resource types (its *inputs*) to a resulting sequence of resource types (its *outputs*)[fn:domcod]: "the Acme webservice" might be a name that relates inputs like an executable for the service, a database, and a service config file to an output web service. We can visualize that as:

[FIG]

Names must be *deterministic*, in the sense that the process must produce equivalent outputs when provided equivalent inputs. This may seem to make them too strict to be useful, but there are two mitigating factors. First, recall that equivalence is a domain-specific notion; depending on how high level the output type equivalences are the name may have quite a bit of leeway in exactly how it instantiates the desired resources. Moreover, names can be *contextual* (or *indexical*), meaning that their output can depend on (some aspect of) the caller's context; "the results of today's prod pipeline run" depends on what "today" means. This is modelled by a *context* resource types at the input, which can be thought of as specific subsets of "the state of the world from some particular perspective"; they can conceptually be instantiated with a (unique) instance from a caller outside the system or forwarded on (possibly after transformations) within it. Because each top-level instantiation is unique, contextual names are essentially unrestricted with respect to determinism, so long as the lack of determinism can be captured in the context.

Names with an empty list of inputs are therefore called *named resources*[fn:points], since they correspond directly to the (unique up to equivalence) resources produced when the name is run.

We can combine names via *substitution*[fn:1-comp], instantiating some input resources of one name with (subset *projections* of) the outputs of some other names (and so on recursively), resulting in a new name. We might visualize "the Acme webservice using the executable compiled from the latest code, the pristine test db, and some provided config file" as:

[FIG]

Which as a whole can be seen as new contextual name taking a config file as an input:

[FIG]

Names are *referentially transparent*[fn:cut-elim], in that we can replace a substitution by "inlining" the result resource rather than referencing it and get the same output (this follows from determinism).

Resource subtyping can be captured in *coercions* (or *upcasts*), names that map a single input to a single output and are operationally noops. The server compliation process coerced the writeable file your editor was using to a readable stream to generate an updated server executable.

Because of determinism, using names forces us to say exactly what we mean. Domain-specificity and contextuality /allow/ us to say exactly what we mean, and no stricter, especially if the contextual inputs are fine-grained. Together, this gives us an expressive specification that lets us rely on names and know what to expect with the resulting resources, across domains, modulo implementation bugs. Determinism also allows for efficient resource instantiation: If we can cheaply determine that the inputs are all equivalent to some previous instantiation (here or elsewhere), we can safely reuse the previous result, and to the extent that contextuality doesn't tie us to a specific machine we can safely distribute the work to other systems and take the result back when done. For named resources in particular, since the inputs are always vacuously equivalent we can aggressively cache and distribute them.

Many names can themselves be cheaply compared for equality by being associated with relatively small byte strings, called their *spelling*, with the semantics that any two names which are spelled the same are the same name. This allows for composed names to be subject to caching without necessarily running intermediate names or even instantiating their results from a cache, since if we know the top-level inputs are equivalent and each name in the chain is equivalent we know the outputs will be equivalent. Spellings typically fall into two categories

+ *Canonical* spellings are short, descriptive character strings. For example, we might have the string ~$HOME~ spell out a contextual name yielding the caller's home directory.
+ *Hashed* spellings are a cryptographic hash of a serialization of (some function of) the data needed to actually run the name. If we substitute some file spelled ~foo~ into some name that compiles C programs, we might spell the resulting name ~sha256("compile-C C11 ${foo}")~. Hashed spellings can omit or transform some of the data from the input to the hash, so long as the name can be considered the name invariantly under that transformation.

[fn:1-cell] The 1-cells of the category.
[fn:domcod] The domain and codomain of the 1-cells. Note that this could in principle be independently extended to a dependent category by allowing the output types to depend on the specific input resources provided or to a codependent category by allowing the inputs to vary depending on how the outputs are used, but there is currently no known practical use case for those.
[fn:points] /These/ are the points of the relevant 0-cell. Not every resource has a name that fits the requirements of names generally, at least not obviously so, so while every named resource corresponds to some resource the converse isn't true.
[fn:1-comp] This is (unbiased) composition of the 1-cells, including tensoring (i.e. projections).
[fn:cut-elim] This is "cut elimination" of the underlying multicategory

**** Technical note: Structural rules

The rules for names given so far imply very strict resource management: Every resource must be used, exactly once, in order. There are some cases where this is necessary for correctness. Consider the case where a name depends on three input streams that gets instantiated with three pipes each filled sequentially by the same process; the first pipe must be completely read from before the process will start filling the second one, so the name must consume it first, and the data streams can be arbitrarily long so they cannot in general be duplicated. In most cases, however, we can relax this through any combination of the following three schemes for *structural names*:

[FIG]

*Weakening* lets you ignore some resource: the name doesn't do anything with its input. *Contracting* lets you duplicate some input: the name copies[fn:ref] the resource it's instantiated with and sends one copy over each output. *Exchanging*, which can also be visualized by simply crossing wires, lets you reorder inputs: the input on the new left wire is forwarded on to the right output wire etc.

By default, all inputs and outputs are eligible for all three schemes. On a case by case basis we can conceptually annotate given inputs or outputs with *substructural restrictions*. Marking an output as *relevant* indicates that the result must be used and thus can't be weakened; marking an input as relevant indicates that the name does in fact use that input (e.g. internally it doesn't weaken it anywhere). Marking an output as *affine* indicates that the result can't be copied and thus can't be contracted; marking an input as affine indicates that the name does not duplicate that input. Marking an output as *ordered* indicates that nothing before it can be used once it's used (if ever) and it can't be used once something after has been used and thus can't be exchanged; marking an input as ordered indicates that the name does not reorder resources around that input[fn:one-sided].

In addition to ensuring correctness in rare cases, these annotations can also be used for optimization. If an input is marked relevant, the caller (or general substitution mechanism) might eagerly prepare the resource for consumption (e.g. starting a socket-activated service) rather than waiting for it to be used, since it will be eventually. If an input is marked affine, the caller might garbage collect the resource once it's used. If it's marked ordered, all resources before the input in question can be discarded/preparations stopped once the input is used, and the input itself discarded once something after it is.

[fn:ref] Often by reference!
[fn:one-sided] In principle we could restrict exchange in only one direction, resulting in a one-way "barrier" to reorders.
*** Reductions
We've already seen how the properties of names allow for efficient resource instantiation and combination. Unfortunately, the efficiency ultimately relies on identifying equivalent inputs, which is not always cheap and sometimes impossible. Consider the compute pipeline. A "run of the pipeline" might depend on the entire pipeline package and then project out the executable for each stage. Since you've changed one module in the pipeline, the whole package has changed. If your change only impacts, say, the last stage of the pipeline, the individual stages might be able to recognize that their executables are unchanged. But after the first stage, this recognition wouldn't result in reuse: the first stage may have output cached results, but other stages may not be able to cheaply detect that the output is the same and so would have to rerun. *Reductions*[fn:2-cell] allow us to convey this kind of information by relating one name to another; once we know that "build the project and project out the first executable" reduces to "this particular named executable resource", we can apply our caching logic to the entire composed chain without ever running any particular unchanged stage:

[FIG]

Reductions compose with each other, including across substitutions and projections[fn:2-comp]; they can be thought of as substitutions at the name level. For example, if we have:

[FIG]

Then we get a composite reduction:

[FIG]

Reductions must preserve determinism. Some trivial reductions come automatically: Any depth of nested substitutions reduces to a substitution where everything is simultaneous[fn:lax], contraction followed by weakening on one of the outputs cancels out to a noop, and a sequence of exchanges that leaves you back where you started cancels. Others are domain-specific, letting you express how your names relate to other names.

Reductions can be determined a priori, just based on the name, or can be identified while the name is being run; a compilation name might run the compilation to completion and then reduce itself to a content-addressed name for the resultant file.

Reductions can effectively change the input requirements; we can drop, duplicate, or rearrange wires so long as we respect substructral restrictions[fn:red-substruct]. Reductions can also *downcast* output types into a more specific type, if we know that the output in the specific cases we've isolated will actually be the right kind of resource. Together, these capabilities allow us to flexibly build names that reuse other names for their work and make that reuse visible to the system as a whole. For example, we could build a TTL cache combinator that takes some name and produces a new name that takes all the same inputs plus the current time and cache state, and either reduces to some named resources (weakening the remaining inputs) if we've run this name recently enough or reduces to the underlying name with the remaining inputs if we haven't (and captures the result for next time)[fn:ml]. Or all of our names that deal with files could delegate the actual file storage to some content-based names and downcast the results to an appropriate specific kind of file, allowing us to identify two different names that result in a file with the same contents as being the same.

[fn:2-cell] The 2-cells. Note that each hom-category is thin for our purposes, i.e. the only relevant 2-dimensional data is whether a reduction exists in a given direction or not
[fn:2-comp] (Unbiased) composition of 2-cells, including vertical, horizontal, and tensoring
[fn:lax] Thus our 1-composition is lax, not even weak
[fn:red-substruct] In particular, we can't drop a relevant wire unless we already used the resource before identifying/following the reduction, we can't retain an affine wire unless we haven't used it before identifying/following the reduction, and the evident but verbose rules for ordered wires apply as well.
[fn:ml] Note that this could be arbitrarily complex; we could e.g. have some ML-based "fuzzy matching" on the inputs and an extra model state input, if we have some learned notion of when results are going to be "close enough" based on the input closeness.
*** Namepsaces
Implicit in the whole discussion so far is that we are describing an open system: you can freely add new resource types, new names, new reductions, so long as they meet the requirements. Unfortunately, proving or enforcing those requirements is in general infeasible. As a result, the system as a whole is conceptually partitioned into multiple *namespaces*, each of which has control over the names and reductions within it but cannot influence names in namespaces that don't (transitively) reference it. You may have set up a namespace for managing your local project checkouts (so you can just tell your editor "open this file in that project"), and that namespace impacts the module compilation that uses the files you edited by determining which files are passed on to the compiler, but outside of names that reference local project names the module compilation namespace is isolated from anything the local project namespace does, including any properties it violates.

Namespaces are also the locus of caching, including distributed caching and reductions. Namespaces can keep previous results in a *store* or *forward* results from another namespace (say, on another machine). A namespace can also identify reductions for any of its names.

In order to have caching/reduction for composite names whose substitutions cross namespace boundaries, we need some way to determine which namespace gets to provide the results or identify the substitutions as well as some trusted mechanism for that namespace to do name equality comparisons from different namespaces. For the first issue, we reduce the name to a fully flattened normal form and work backwards from the final outputs, letting the relevant namespace determine if it knows of a reduction or has a cached result for the whole input graph up to that point[fn:anywhere]. For the second issue, we can treat namespaces as a resource type and introduce a *namespace of namespaces*, i.e. a namespace whose names produce namespaces. Then each namespace can have namespaces it trusts to name other namespaces, and confirm with the trusted namespaces that a given name belongs where it claims, and include the spelling of the input's namespace in the input's spelling. This can also be used for overlaying optimization or instrumentation; we might have a namespace of namespaces that says "for any name in the namespaces I expose, I'm first going to check this reduction cache I trust to see if it reduces, and only forward on to the underlying namespace if not", which would among other things allow different users on the same machine to have their own trusted 3rd party caches without requiring mutual trust. This can also be used to bootstrap the system; much like filenames are usually releative to some ambient root or current directory, most names will be relative to some ambient namespace namespace that provides the default set of namespaces for the user or the system.

[fn:anywhere] Technically we could safely allow namespaces to reduce based on what comes /after/ as well. But until a use case arises this allows for a much more straightforward and efficient execution algorithm.
** TODO The implementation
Elvysh provides several core components to make this model real:

+ *Interfaces* include protocols between namespaces (or between the user and namespaces), 
** TODO L Core technical components/architecture implementing the model
+ GC
+ Centralize reductions/per user
+ Lazy/incomplete
+ Value-like resources
+ Pull/push
+ Provisional results
+ Scheduler/resource optimization/global optimization
+ Priority queue caps
+ Well-known projections
+ Naming
  + Hashing vs authoritative name server, what to hash
+ Trusted ns vs root ns
** Applications
In this section, we'll survey a non-exhaustive list of possible applications of elvysh. Keep in mind that a key feature is that names and substitution can operate across domains, so we should expect synergy between these when multiple are implemented!

*** Content-addressed storage
Any time we have some resource type defined by its contents and those contents are cheap enough to enumerate, we can build a content-addressed namespace around it. The typical example is immutable files: given any file, we can build a named resource whose contents match that file's at one read-through and whose spelling is a direct hash of the contents. We can also build contextual resources based on handles to the resource in question, e.g. we may have a name ~stdin~ that takes file descriptor 0 from the context, starts reading through it and saving the file to the store, and when it's done emits a reduction to the named resource corresponding to the file just saved.

There are many many systems implementing content-addressed storage for files, including git's object store and the IPFS distributed file system. These could be reimplemented as elvysh namespaces, or in cases like IPFS elvysh may reuse its protocols for effective distribution and storage.

It is expected that many namespaces will have their names reduce to some content-addressed named resource when it's feasible to do so, as this allows reuse of the underlying storage mechanisms and enables reuse when two potentially very different processes result in the same outcome.
*** Package management
Fully reproducible efficiently shared package environments are a core use case of elvysh. The seed of elvysh's design comes from Nix, a system that provides many of the benefits of elvysh specific to the package management domain:

+ Nix has content-addressed storage, extended from regular files to the subset of directories that is needed to represent full packages
+ Nix has a mechanism for serializing package build scripts that captures package dependencies as well as the commands to run, which it then hashes to get an identifier for the resulting package

Together with an isolation mechanism to ensure that nothing unlisted is used, this allows for a package's identifier to correspond exactly to the steps required to produce it from a base set of content-addressed files. Elvysh can extend this by:

+ Having higher level notions of "package", e.g. a resource type for a "cross-compiled package" that treats as equivalent two packages that use otherwise identical inputs but one was cross-compiled and one native
+ Having multiple namespaces allowing different naming rules and instantiation processes; Nix's are appropriately strict given the need to capture arbitrary package build scripts and ensure determinism, but are overkill and inefficient for many use cases.
+ Having a representation for unsubstituted names with inputs that can be reused in different combinations
+ Allowing fine-grained contextuality, for cases where full purity is not appropriate
+ Having reductions[fn:fixed], including the so-called "intensional store" and recursive Nix
+ Enabling optimizations by elvysh-aware components, such as early use of partially-instantiated packages and more efficient runtime dependency identification
+ Allowing the package environments themselves to be first-class resources, enabling higher level operations like "install a package into my user env" to be directly represented in the system
+ Allowing for secret files to exist in appropriately restricted namespaces, when building system configurations.

[fn:fixed] Arguably Nix already has reductions in the single case of fixed-output derivations; they (statically) reduce to the fixed output file with the appropriate hash. This allows for e.g. nix-prefetch-url to work without running a derivation.
*** Unison
Unison is an in-development programming language whose core features can be seen as special cases of Elvysh. Unison has immutable content-addressed /expressions/, based on hashing of the language's AST (up to alpha equivalence). This allows for:

+ Implicit incremental compilation/evaluation. When Unison needs to evaluate some expression, it can very cheaply determine if it already has, or if it has evaluated some subexpression, and only needs to compile and compute what has changed.
+ Exact dependency management within the Unison unverse. Any definitions you depend on from some other project are fully content-addressed, with no room for naming conflicts (though of course if two parts of your code base use two "versions" of the same type, they won't automatically interconvert)
+ Native distributed computation. Code and computation can be straightforwardly distributed based on the desired compute graph, since we can easily determine if some (subset) of code already exists on a given node or some subset of the computation has already been evaluated, and purity of the language ensures it's safe to combine the results from any node.
+ Cheap correct renaming. Human-visible names are simple mappings to the actual underlying content-addressed name that can be easily updated, and in fact different users can have different names for the same expressions without issue.

Elvysh can extend this by:

+ Combining the language functionalities with package management to give Unison an FFI that has the same easy transparent dependency management and preserves Unison's properties
+ Enabling some form of this functionality for arbitrary languages. Without significant work this would have to be restricted to the module level, but it would still allow the implicit recompilation and code distribution for any language
  + In any context where we can guarantee evaluation is pure (e.g. safe Haskell, or a trusted promise), we can cache evaluation as well
+ Allowing alternate equivalence classes of expressions. If you update some function to make it more efficient but can prove (or, if trusted, assert) that it has the same behavior, the evaluation cache could use results from either version and older code could be automatically upgraded
*** Service orchestration
By treating services as resources, elvysh can provide an immutable infrastructure-style approach toward service orchestration. Inter-service dependencies can be modelled as inputs, which are substituted by giving one service a capability to another; if we depend on a service that is the same as one already deployed, we don't need to deploy it again. This shares some properties with Nelson, an orchestration tool that leverages semantic versioning and explicitly configured dependencies to achieve the same outcome in a container-based environment.
*** Compute pipelines
By modelling computation results as resources, individual stages as primitive names, and compute graphs as composed names, we can automatically orchestrate arbitrarily complex compute pipelines with safe caching and reuse. The same computation definition can be easily transformed to run locally threaded in-process or across hundreds of machines. We can capture batch processes or system state in contextual inputs that then reduce to non-contextual ones once accessed, thus automatically sharing work without an a priori notion of what has or hasn't changed.
*** Continuous integration
A specification for continuous integration can be a name that composes all of the relevant projects together, and by combining contextuality and reduction we can capture notions like "the latest version of each dependency" without doing unecessary new work. Test results can be seen as their own resource and potentially named independently of build products, with parallel computation possible if applicable.
** Engineering standards
As an aspiring foundational component of nearly every system, it is vital that elvysh be engineered to very high standards. Specific principles include:

+ Specification. The system must have clear precise semantics, library interfaces must be fully documented, formats and protocols spelled out in detail. It should be possible based on specifications alone to reimplement any part of the system compatibly, or even the whole.
+ Composability. The system must be made up of composable primitives that serve a single semantic purpose and can be combined in arbitrary ways so long as the semantics are respected. Wherever possible this applies even across versions; we do not assume everything running was compiled against the same master codebase. Users should be able to build arbitrary domain-specific systems on top of the core that can all interact. Elvysh may include some opinionated "best practice" combinations of components, but cannot assume that those components are always used in that configuration. Elvysh provides mechanism, not policy. Elvysh provides code for reuse wherever possible.
+ Observability. Elvysh's users and developers need to be able to understand the behavior and state of the running system, without reinstrumentation or rebuilding. Elvysh components can build up and emit rich domain-specific structured event information at every step, which can be sampled and correlated across components to aid in debugging, understand user behavior, identify optimization opportunities, etc.
+ Verification. Leveraging as appropriate peer review, testing, fuzzing, formal specification and model checking, formal implementation validation, run-time observation, etc., we want to continually iterate toward ensuring the system is sensibly specified and properly implemented.
+ Security. Elvysh has security built in from the beginning, with clear boundaries between systems, a model assuming mutually untrusted implementations and users, and applying least privilege throughout. Wherever possible based on the underlying system primitives, elvysh uses object capability-style access control, and where not possible it is emulated. In addition to eliminating whole classes of privilege escalation bugs, this makes for a much cleaner programming model when coordinating between many systems.
+ Compatibility. Elvysh is designed for future enhancements wherever possible, and adheres to strict protocol and API versioning to ensure any backwards incompatibilities that must happen are caught early.
+ Portability. The core components should work on most platforms, and cross-platform interaction should work smoothly.
* Scarf tools
Elvysh alone is not a tool; it has no frontend, just interfaces. Users need some way to actually /use/ the functionality elvysh provides, and Scarf can provide those.
** Why Elvysh and Scarf?
There is remarkable synergy between elvysh's project goals and Scarf's:

+ Elvysh's functionality will incentivize its prospective users to install Scarf's tooling to get access; Scarf's functionality will lead its prospective users into the elvysh ecosystem
+ Elvysh provides a distribution model that can form the basis of Scarf's commercial platform; Scarf's commercial platform will provide the infrastructure and developer incentives to implement the model
+ Elvysh enables a uniform package management experience across ecosystems and platforms; Scarf can bring multiple ecosystems and platforms to the table
+ Elvysh's observability (or a namespace namespace that repoints URL lookups through Scarf) can be leveraged to collect information about how and where different packages are used; Scarf's collection of that information can guide elvysh's development to best meet the needs of its users
+ Elvysh and Scarf are both heavily aligned around maintainers and developers, providing them the capabilities to effectively build and distribute their systems.

Each project would hugely benefit from the success of the other, and working as one we can drive that success together.
** Potential user workflow
The long term vision has elvysh sinking into the background for the user, with all relevant tools having Scarf-provided functionality to make them natively elvysh aware. Your editor, your compiler, your shell, your browser, your application launcher all just understand elvysh names and combine them appropriately, with all the analytics available to Scarf. But that's an endpoint, not a starting point. Our initial target is package management, developer environments, and compilation for Rust and Haskell developers.

Wherever possible, we want to match the existing behaviors users are already used to. They should be able to install and uninstall packages by name, do the equivalent of ~cabal build~, use ~Cargo.lock~ for dependency pinning, etc. In order to integrate in with existing tooling as much as possible (e.g. editor integrations), these will ideally be drop-in command line replacements that can be added to the PATH. The Scarf tools would then translate those commands into appropriate Elvysh names under the hood and update the results appropriately. However, some cases may require more elvysh-specific specification, such as:

+ Declarative user environment specifications
+ Using a specific GHC version
+ Dependencies on internal projects (including local checkouts vs internal releases)
+ Cross-language dependencies
+ Ensuring everyone on the team is using the exact same package set all the way down

In these cases, we will endeavor to provide simple domain-specific configurations. A declarative user environment could consist of, say, a git revision for the package repository and a list of package names from that repository, and we could even have the command line tool to add/remove packages keep such a configuration up to date. While the configuration languages are developing, however, we will also provide "escape hatches" to provide programs that directly speak elvysh's protocols or, in the case of packages, Nix code to manipulate the package set. The goal will be to identify wherever these escape hatches are needed and find a semantically appropriate way to represent that in the higher level language, in particular limiting the amount of Nix code that needs to be written or understood as much as possible.

Over time, we will endeavor to move as much as possible into upstream components (e.g. Cargo should speak elvysh, possibly through a plugin) and the rest into standalone tools that are integrated into the rest of the ecosystem (e.g. your editor can understand the Scarf project environment file and set up its local environment appropriately).

In the longer run, ideally we'd have as much of this as possible happen implicitly/on-demand. For example, we might have:

+ "command not found"-style functionality to automatically fetch a package not yet installed, optionally including it as part of the environment and kept up to date with the rest
+ Background rebuilding, or even the entire CI/CD process, on file save
+ Shell automatically entering the environment for a specific project when opened
** Accompanying infrastructure
In addition to elvysh itself and the frontends, some additional infrastructure will be needed to fully realize the value:

+ Scarf needs some way to capture and analyze the information identified by the frontends
+ Scarf needs a forwarding server if it's going to act as a registry
+ Scarf likely needs a cache, of packages at least

Over time, Scarf could also offer other elvysh-powered services, such as CI or even a PaaS offering.
* TODO Project plan
** L Roadmap with technical and functional milestones
Nixpkgs compat:
  Add files
    direct add to store
    Builtin drvs
    recursive vs flat
  References
  Run drvs
    Basic execution
    Funky special features
    Serialize drvs
    Intensional?
    Recursive?
    Remote?
    Substitution?
  GC
  nixexpr interface
    Basic eval
    String context
    path
    derivationStrict
    funky builtins?
    Interface to other stores?
  nixenv/profile interface
    GC connected to profile dirs
Haskell
  Individual module
  Whole package
  Deps?
  nix bidi interaction
Interface
  C
  Rust
  Haskell
Documentation
  Reference/protocols
  Tutorials
  Cookbook/how-to
Formal modelling
Portability?
** L Detailed review of each phase
** L Timelines
** S Opportunities for parallelism/team work
** M Proposal for messaging/marketing to existing Nix and developer tool communities
** M Expected limitations of each milestone and the completed initial product
** S Future opportunities
* TODO Proposed terms of employment
** TODO Governance
Owner's interest, maintainers decision
